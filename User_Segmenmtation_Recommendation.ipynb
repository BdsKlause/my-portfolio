{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPahi7eSC26ej0hp3i7wP9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BdsKlause/my-portfolio/blob/main/User_Segmenmtation_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fhJr0Et48Kvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# environment setup"
      ],
      "metadata": {
        "id": "u7lQuVkj8LE5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knRrXMuE7AvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13b6cae-0142-409f-caff-46090bfdeba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install and import necessary libraries\n",
        "!pip install scikit-surprise \"numpy<2\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from surprise import Dataset, Reader, KNNBasic, accuracy\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('ggplot')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "print(\"Libraries installed and imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "zrm3_9RJIK-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the MovieLens 100K dataset\n",
        "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
        "!unzip ml-100k.zip\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading MovieLens 100K dataset...\")\n",
        "\n",
        "# Load ratings data\n",
        "ratings = pd.read_csv('ml-100k/u.data', sep='\\t',\n",
        "                     names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
        "\n",
        "# Load movie information\n",
        "movies = pd.read_csv('ml-100k/u.item', sep='|', encoding='latin-1',\n",
        "                    names=['item_id', 'title', 'release_date', 'video_release_date',\n",
        "                          'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
        "                          'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
        "                          'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
        "                          'Thriller', 'War', 'Western'])\n",
        "\n",
        "# Load user information\n",
        "users = pd.read_csv('ml-100k/u.user', sep='|',\n",
        "                   names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Ratings: {ratings.shape}\")\n",
        "print(f\"Users: {users.shape}\")\n",
        "print(f\"Movies: {movies.shape}\")"
      ],
      "metadata": {
        "id": "Q0J1g5K8I9Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory data analysis"
      ],
      "metadata": {
        "id": "y2yh0FIiJKTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic dataset info\n",
        "print(\"=== DATASET OVERVIEW ===\")\n",
        "print(\"\\nFirst few ratings:\")\n",
        "print(ratings.head())\n",
        "\n",
        "print(\"\\nFirst few users:\")\n",
        "print(users.head())\n",
        "\n",
        "print(\"\\nRating distribution:\")\n",
        "print(ratings['rating'].value_counts().sort_index())\n",
        "\n",
        "# Visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Rating distribution\n",
        "ratings['rating'].value_counts().sort_index().plot(kind='bar', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Rating Distribution')\n",
        "axes[0, 0].set_xlabel('Rating')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "\n",
        "# User age distribution\n",
        "users['age'].hist(bins=20, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Age Distribution')\n",
        "axes[0, 1].set_xlabel('Age')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "\n",
        "# Gender distribution\n",
        "users['gender'].value_counts().plot(kind='bar', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Gender Distribution')\n",
        "axes[1, 0].set_xlabel('Gender')\n",
        "axes[1, 0].set_ylabel('Count')\n",
        "\n",
        "# Ratings per user\n",
        "user_rating_counts = ratings['user_id'].value_counts()\n",
        "user_rating_counts.hist(bins=30, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Number of Ratings per User')\n",
        "axes[1, 1].set_xlabel('Number of Ratings')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Occupation distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "users['occupation'].value_counts().plot(kind='bar')\n",
        "plt.title('Occupation Distribution')\n",
        "plt.xlabel('Occupation')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JIpvrjBIJREP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering for user clustering"
      ],
      "metadata": {
        "id": "emo4xe4HJ5CL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create user features for clustering\n",
        "print(\"Creating user features for clustering...\")\n",
        "\n",
        "# User rating behavior features\n",
        "user_stats = ratings.groupby('user_id').agg({\n",
        "    'rating': ['count', 'mean', 'std']\n",
        "}).reset_index()\n",
        "user_stats.columns = ['user_id', 'rating_count', 'rating_mean', 'rating_std']\n",
        "\n",
        "# Merge with demographic data\n",
        "user_features = pd.merge(users, user_stats, on='user_id')\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "user_features = pd.get_dummies(user_features, columns=['gender', 'occupation'])\n",
        "\n",
        "# Select numerical features for clustering\n",
        "feature_columns = ['age', 'rating_count', 'rating_mean', 'rating_std'] + \\\n",
        "                 [col for col in user_features.columns if col.startswith('gender_') or col.startswith('occupation_')]\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "user_features_scaled = scaler.fit_transform(user_features[feature_columns])\n",
        "\n",
        "print(f\"Created {len(feature_columns)} features for clustering\")\n",
        "print(f\"Feature matrix shape: {user_features_scaled.shape}\")"
      ],
      "metadata": {
        "id": "wMfO5jEpJ-_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality reduction"
      ],
      "metadata": {
        "id": "eZkTwm66KFwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA for visualization\n",
        "print(\"Applying PCA for dimensionality reduction...\")\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "user_features_pca = pca.fit_transform(user_features_scaled)\n",
        "\n",
        "# Plot explained variance\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "         pca.explained_variance_ratio_.cumsum(), marker='o')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(user_features_pca[:, 0], user_features_pca[:, 1], alpha=0.6)\n",
        "plt.title('Users in 2D PCA Space')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.3f}\")"
      ],
      "metadata": {
        "id": "udi_WmJUKKN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# determine optimal no. of clusters"
      ],
      "metadata": {
        "id": "-k1ynP_TKglH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find optimal number of clusters using elbow method and silhouette score\n",
        "print(\"Determining optimal number of clusters...\")\n",
        "\n",
        "inertia = []\n",
        "silhouette_scores = []\n",
        "db_scores = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(user_features_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "    if k > 1:  # Silhouette score requires at least 2 clusters\n",
        "        silhouette_scores.append(silhouette_score(user_features_scaled, kmeans.labels_))\n",
        "        db_scores.append(davies_bouldin_score(user_features_scaled, kmeans.labels_))\n",
        "    print(f\"Completed clustering for k={k}\")\n",
        "\n",
        "# Plot results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "axes[0].plot(k_range, inertia, marker='o')\n",
        "axes[0].set_title('Elbow Method')\n",
        "axes[0].set_xlabel('Number of Clusters')\n",
        "axes[0].set_ylabel('Inertia')\n",
        "\n",
        "axes[1].plot(range(2, 11), silhouette_scores, marker='o')\n",
        "axes[1].set_title('Silhouette Score')\n",
        "axes[1].set_xlabel('Number of Clusters')\n",
        "axes[1].set_ylabel('Silhouette Score')\n",
        "\n",
        "axes[2].plot(range(2, 11), db_scores, marker='o')\n",
        "axes[2].set_title('Davies-Bouldin Score')\n",
        "axes[2].set_xlabel('Number of Clusters')\n",
        "axes[2].set_ylabel('DB Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select optimal k based on silhouette score\n",
        "optimal_k = range(2, 11)[np.argmax(silhouette_scores)]\n",
        "print(f\"Optimal number of clusters: {optimal_k} (based on silhouette score)\")"
      ],
      "metadata": {
        "id": "t3zawgLRKlJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-MEANS clustering"
      ],
      "metadata": {
        "id": "WjGMuVBBKuXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means with optimal k\n",
        "print(f\"Applying K-Means clustering with k={optimal_k}...\")\n",
        "\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "user_features['cluster'] = kmeans.fit_predict(user_features_scaled)\n",
        "\n",
        "# Visualize clusters in PCA space\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(user_features_pca[:, 0], user_features_pca[:, 1],\n",
        "                     c=user_features['cluster'], cmap='viridis', alpha=0.6)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.title('User Clusters in PCA Space')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()\n",
        "\n",
        "print(\"Clustering completed successfully!\")"
      ],
      "metadata": {
        "id": "YH7zNPn5Kya2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze cluster characteristics\n",
        "print(\"Analyzing cluster characteristics...\")\n",
        "\n",
        "cluster_analysis = user_features.groupby('cluster').agg({\n",
        "    'age': 'mean',\n",
        "    'rating_count': 'mean',\n",
        "    'rating_mean': 'mean',\n",
        "    'rating_std': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "# Add cluster sizes\n",
        "cluster_sizes = user_features['cluster'].value_counts().sort_index()\n",
        "cluster_analysis['size'] = cluster_sizes.values\n",
        "\n",
        "print(\"Cluster Characteristics:\")\n",
        "print(cluster_analysis)\n",
        "\n",
        "# Visualize cluster characteristics\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "cluster_analysis['age'].plot(kind='bar', ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Average Age by Cluster')\n",
        "axes[0, 0].set_xlabel('Cluster')\n",
        "axes[0, 0].set_ylabel('Age')\n",
        "\n",
        "cluster_analysis['rating_count'].plot(kind='bar', ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Average Rating Count by Cluster')\n",
        "axes[0, 1].set_xlabel('Cluster')\n",
        "axes[0, 1].set_ylabel('Rating Count')\n",
        "\n",
        "cluster_analysis['rating_mean'].plot(kind='bar', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Average Rating by Cluster')\n",
        "axes[1, 0].set_xlabel('Cluster')\n",
        "axes[1, 0].set_ylabel('Rating')\n",
        "\n",
        "cluster_analysis['size'].plot(kind='bar', ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Cluster Size')\n",
        "axes[1, 1].set_xlabel('Cluster')\n",
        "axes[1, 1].set_ylabel('Number of Users')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze gender distribution across clusters\n",
        "if 'gender_M' in user_features.columns:\n",
        "    gender_cluster = pd.crosstab(user_features['cluster'], user_features['gender_M'])\n",
        "    gender_cluster.columns = ['Female', 'Male']\n",
        "    gender_cluster_percent = gender_cluster.div(gender_cluster.sum(axis=1), axis=0) * 100\n",
        "\n",
        "    print(\"\\nGender Distribution by Cluster (%):\")\n",
        "    print(gender_cluster_percent.round(2))\n",
        "\n",
        "    gender_cluster_percent.plot(kind='bar', stacked=True)\n",
        "    plt.title('Gender Distribution by Cluster')\n",
        "    plt.xlabel('Cluster')\n",
        "    plt.ylabel('Percentage')\n",
        "    plt.legend(title='Gender')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gUmbvKjbK-Ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reccommendation systems build"
      ],
      "metadata": {
        "id": "3eDvw9XiLLmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for recommendation systems\n",
        "print(\"Building recommendation systems...\")\n",
        "\n",
        "# Create reader and dataset for surprise\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(ratings[['user_id', 'item_id', 'rating']], reader)\n",
        "\n",
        "# Split data for evaluation\n",
        "trainset, testset = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set: {trainset.shape[0]} records\")\n",
        "print(f\"Test set: {testset.shape[0]} records\")\n",
        "\n",
        "# Build baseline recommendation system (without clustering)\n",
        "print(\"Building baseline recommendation system...\")\n",
        "\n",
        "# User-based collaborative filtering\n",
        "sim_options = {'name': 'cosine', 'user_based': True}\n",
        "baseline_algo = KNNBasic(sim_options=sim_options)\n",
        "\n",
        "# Train on the entire dataset for simplicity\n",
        "trainset_full = data.build_full_trainset()\n",
        "baseline_algo.fit(trainset_full)\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_predictions = baseline_algo.test(testset[['user_id', 'item_id', 'rating']].values)\n",
        "baseline_rmse = accuracy.rmse(baseline_predictions, verbose=False)\n",
        "baseline_mae = accuracy.mae(baseline_predictions, verbose=False)\n",
        "\n",
        "print(f\"Baseline RMSE: {baseline_rmse:.4f}, MAE: {baseline_mae:.4f}\")\n",
        "\n",
        "# Build cluster-based recommendation systems\n",
        "print(\"Building cluster-based recommendation systems...\")\n",
        "\n",
        "cluster_models = {}\n",
        "cluster_performance = {}\n",
        "\n",
        "for cluster_id in user_features['cluster'].unique():\n",
        "    # Get users in this cluster\n",
        "    cluster_users = user_features[user_features['cluster'] == cluster_id]['user_id']\n",
        "\n",
        "    # Get ratings for these users\n",
        "    cluster_ratings = ratings[ratings['user_id'].isin(cluster_users)]\n",
        "\n",
        "    if len(cluster_ratings) > 20:  # Only create model if enough data\n",
        "        # Create and train model for this cluster\n",
        "        cluster_data = Dataset.load_from_df(cluster_ratings[['user_id', 'item_id', 'rating']], reader)\n",
        "        cluster_trainset = cluster_data.build_full_trainset()\n",
        "\n",
        "        cluster_algo = KNNBasic(sim_options=sim_options)\n",
        "        cluster_algo.fit(cluster_trainset)\n",
        "\n",
        "        cluster_models[cluster_id] = cluster_algo\n",
        "\n",
        "        # Evaluate on test data for this cluster\n",
        "        cluster_test_data = testset[testset['user_id'].isin(cluster_users)]\n",
        "        if len(cluster_test_data) > 5:\n",
        "            cluster_predictions = cluster_algo.test(cluster_test_data[['user_id', 'item_id', 'rating']].values)\n",
        "            cluster_rmse = accuracy.rmse(cluster_predictions, verbose=False)\n",
        "            cluster_mae = accuracy.mae(cluster_predictions, verbose=False)\n",
        "\n",
        "            cluster_performance[cluster_id] = {\n",
        "                'rmse': cluster_rmse,\n",
        "                'mae': cluster_mae,\n",
        "                'size': len(cluster_users),\n",
        "                'test_size': len(cluster_test_data)\n",
        "            }\n",
        "            print(f\"Cluster {cluster_id}: RMSE={cluster_rmse:.4f}, MAE={cluster_mae:.4f}\")\n",
        "\n",
        "# Calculate weighted average performance for cluster-based approach\n",
        "if cluster_performance:\n",
        "    total_test_size = sum([info['test_size'] for info in cluster_performance.values()])\n",
        "    weighted_rmse = sum([info['rmse'] * info['test_size'] for info in cluster_performance.values()]) / total_test_size\n",
        "    weighted_mae = sum([info['mae'] * info['test_size'] for info in cluster_performance.values()]) / total_test_size\n",
        "\n",
        "    print(f\"\\nCluster-based Weighted RMSE: {weighted_rmse:.4f}, MAE: {weighted_mae:.4f}\")\n",
        "    print(f\"Improvement in RMSE: {(baseline_rmse - weighted_rmse) / baseline_rmse * 100:.2f}%\")\n",
        "    print(f\"Improvement in MAE: {(baseline_mae - weighted_mae) / baseline_mae * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"No cluster models were created due to insufficient data in clusters.\")"
      ],
      "metadata": {
        "id": "Y3RSdLdxLTj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compare recommendation system"
      ],
      "metadata": {
        "id": "2Ww6J4q6L_XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare performance visually\n",
        "if cluster_performance:\n",
        "    cluster_ids = list(cluster_performance.keys())\n",
        "    cluster_rmses = [cluster_performance[cid]['rmse'] for cid in cluster_ids]\n",
        "    cluster_maes = [cluster_performance[cid]['mae'] for cid in cluster_ids]\n",
        "    cluster_sizes = [cluster_performance[cid]['size'] for cid in cluster_ids]\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # RMSE comparison\n",
        "    x_pos = np.arange(len(cluster_ids))\n",
        "    axes[0].bar(x_pos, cluster_rmses, alpha=0.7, label='Cluster-based')\n",
        "    axes[0].axhline(y=baseline_rmse, color='r', linestyle='--', label='Baseline')\n",
        "    axes[0].set_xlabel('Cluster')\n",
        "    axes[0].set_ylabel('RMSE')\n",
        "    axes[0].set_title('RMSE Comparison by Cluster')\n",
        "    axes[0].set_xticks(x_pos)\n",
        "    axes[0].set_xticklabels(cluster_ids)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # MAE comparison\n",
        "    axes[1].bar(x_pos, cluster_maes, alpha=0.7, label='Cluster-based')\n",
        "    axes[1].axhline(y=baseline_mae, color='r', linestyle='--', label='Baseline')\n",
        "    axes[1].set_xlabel('Cluster')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].set_title('MAE Comparison by Cluster')\n",
        "    axes[1].set_xticks(x_pos)\n",
        "    axes[1].set_xticklabels(cluster_ids)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Analyze relationship between cluster size and performance\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axes[0].scatter(cluster_sizes, cluster_rmses)\n",
        "    axes[0].set_xlabel('Cluster Size')\n",
        "    axes[0].set_ylabel('RMSE')\n",
        "    axes[0].set_title('Cluster Size vs RMSE')\n",
        "\n",
        "    axes[1].scatter(cluster_sizes, cluster_maes)\n",
        "    axes[1].set_xlabel('Cluster Size')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].set_title('Cluster Size vs MAE')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xntLuB5sMEm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate sample reccomendations"
      ],
      "metadata": {
        "id": "obsirWeiMXFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample recommendations for different user segments\n",
        "print(\"Generating sample recommendations...\")\n",
        "\n",
        "def get_top_n_recommendations(algo, user_id, n=5):\n",
        "    # Get all items\n",
        "    all_items = ratings['item_id'].unique()\n",
        "\n",
        "    # Get items the user has already rated\n",
        "    rated_items = ratings[ratings['user_id'] == user_id]['item_id'].values\n",
        "\n",
        "    # Predict ratings for unrated items\n",
        "    predictions = []\n",
        "    for item_id in all_items:\n",
        "        if item_id not in rated_items:\n",
        "            pred = algo.predict(user_id, item_id)\n",
        "            predictions.append((item_id, pred.est))\n",
        "\n",
        "    # Sort by predicted rating and get top N\n",
        "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
        "    return predictions[:n]\n",
        "\n",
        "# Select sample users from different clusters\n",
        "sample_users = []\n",
        "for cluster_id in user_features['cluster'].unique():\n",
        "    cluster_users = user_features[user_features['cluster'] == cluster_id]['user_id']\n",
        "    if len(cluster_users) > 0:\n",
        "        sample_users.append(cluster_users.iloc[0])\n",
        "\n",
        "# Get recommendations for sample users\n",
        "for user_id in sample_users[:3]:  # Show for first 3 users\n",
        "    cluster_id = user_features[user_features['user_id'] == user_id]['cluster'].values[0]\n",
        "\n",
        "    print(f\"\\n=== Recommendations for User {user_id} (Cluster {cluster_id}) ===\")\n",
        "\n",
        "    # Get baseline recommendations\n",
        "    baseline_recs = get_top_n_recommendations(baseline_algo, user_id, 5)\n",
        "    print(\"Baseline Recommendations:\")\n",
        "    for i, (item_id, rating) in enumerate(baseline_recs, 1):\n",
        "        movie_title = movies[movies['item_id'] == item_id]['title'].values[0]\n",
        "        print(f\"{i}. {movie_title} (predicted rating: {rating:.2f})\")\n",
        "\n",
        "    # Get cluster-based recommendations if available\n",
        "    if cluster_id in cluster_models:\n",
        "        cluster_algo = cluster_models[cluster_id]\n",
        "        cluster_recs = get_top_n_recommendations(cluster_algo, user_id, 5)\n",
        "\n",
        "        print(\"\\nCluster-based Recommendations:\")\n",
        "        for i, (item_id, rating) in enumerate(cluster_recs, 1):\n",
        "            movie_title = movies[movies['item_id'] == item_id]['title'].values[0]\n",
        "            print(f\"{i}. {movie_title} (predicted rating: {rating:.2f})\")\n",
        "\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "vVeSP0bfMhzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final analysis and conclusion"
      ],
      "metadata": {
        "id": "YXpRv9GPMxMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final analysis and conclusions\n",
        "print(\"=\"*60)\n",
        "print(\"FINAL ANALYSIS AND CONCLUSIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"- Users: {len(users)}\")\n",
        "print(f\"- Movies: {len(movies)}\")\n",
        "print(f\"- Ratings: {len(ratings)}\")\n",
        "\n",
        "print(f\"\\nClustering Results:\")\n",
        "print(f\"- Optimal number of clusters: {optimal_k}\")\n",
        "print(f\"- Cluster sizes: {dict(cluster_analysis['size'].astype(int).to_dict())}\")\n",
        "\n",
        "if cluster_performance:\n",
        "    print(f\"\\nPerformance Comparison:\")\n",
        "    print(f\"- Baseline RMSE: {baseline_rmse:.4f}\")\n",
        "    print(f\"- Cluster-based Weighted RMSE: {weighted_rmse:.4f}\")\n",
        "    print(f\"- Improvement: {(baseline_rmse - weighted_rmse) / baseline_rmse * 100:.2f}%\")\n",
        "\n",
        "print(f\"\\nKey Findings:\")\n",
        "print(\"1. User segmentation through clustering can improve recommendation accuracy\")\n",
        "print(\"2. Different user segments have distinct characteristics and preferences\")\n",
        "print(\"3. Cluster-specific models perform differently across segments\")\n",
        "\n",
        "print(f\"\\nRecommendations for Implementation:\")\n",
        "print(\"1. Use clustering to identify distinct user segments\")\n",
        "print(\"2. Develop segment-specific recommendation models\")\n",
        "print(\"3. Regularly update clusters as user behavior evolves\")\n",
        "print(\"4. Consider hybrid approaches that combine global and segment-specific models\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MINI-PROJECT COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "sFhRVDjAM6Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your notebook and results\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save a copy to your Google Drive\n",
        "!cp \"/content/User_Segmentation_Recommendation_System.ipynb\" \"/content/drive/My Drive/\"\n",
        "\n",
        "print(\"Notebook saved to Google Drive!\")"
      ],
      "metadata": {
        "id": "sodBKbHFNWvk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}